{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNU5z3hVSUPfpIbQx/r3jof",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahalaxmy/NLP/blob/main/pos_tag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8Gcien0gGEe"
      },
      "outputs": [],
      "source": [
        "\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWO6MvcHgJD3",
        "outputId": "a794d490-f0ab-4b34-9b1f-75dbc5ddca23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph =  \"The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language. It supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities.[4] It was developed by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania.[5] NLTK includes graphical demonstrations and sample data. It is accompanied by a book that explains the underlying concepts behind the language processing tasks supported by the toolkit,[6] plus a cookbook.\""
      ],
      "metadata": {
        "id": "MlD2RDXDgJGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "M6Kb6M2ugJJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvnFb8N1gJMj",
        "outputId": "31e87f81-d8e3-4201-9460-66a23bedc5e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language.',\n",
              " 'It supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities.',\n",
              " '[4] It was developed by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania.',\n",
              " '[5] NLTK includes graphical demonstrations and sample data.',\n",
              " 'It is accompanied by a book that explains the underlying concepts behind the language processing tasks supported by the toolkit,[6] plus a cookbook.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.word_tokenize(paragraph)\n",
        "words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBe_SzsDg61R",
        "outputId": "c2e08540-5c52-4d70-928c-0e4910f8b943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Toolkit',\n",
              " ',',\n",
              " 'or',\n",
              " 'more',\n",
              " 'commonly',\n",
              " 'NLTK',\n",
              " ',',\n",
              " 'is',\n",
              " 'a',\n",
              " 'suite',\n",
              " 'of',\n",
              " 'libraries',\n",
              " 'and',\n",
              " 'programs',\n",
              " 'for',\n",
              " 'symbolic',\n",
              " 'and',\n",
              " 'statistical',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " '(',\n",
              " 'NLP',\n",
              " ')',\n",
              " 'for',\n",
              " 'English',\n",
              " 'written',\n",
              " 'in',\n",
              " 'the',\n",
              " 'Python',\n",
              " 'programming',\n",
              " 'language',\n",
              " '.',\n",
              " 'It',\n",
              " 'supports',\n",
              " 'classification',\n",
              " ',',\n",
              " 'tokenization',\n",
              " ',',\n",
              " 'stemming',\n",
              " ',',\n",
              " 'tagging',\n",
              " ',',\n",
              " 'parsing',\n",
              " ',',\n",
              " 'and',\n",
              " 'semantic',\n",
              " 'reasoning',\n",
              " 'functionalities',\n",
              " '.',\n",
              " '[',\n",
              " '4',\n",
              " ']',\n",
              " 'It',\n",
              " 'was',\n",
              " 'developed',\n",
              " 'by',\n",
              " 'Steven',\n",
              " 'Bird',\n",
              " 'and',\n",
              " 'Edward',\n",
              " 'Loper',\n",
              " 'in',\n",
              " 'the',\n",
              " 'Department',\n",
              " 'of',\n",
              " 'Computer',\n",
              " 'and',\n",
              " 'Information',\n",
              " 'Science',\n",
              " 'at',\n",
              " 'the',\n",
              " 'University',\n",
              " 'of',\n",
              " 'Pennsylvania',\n",
              " '.',\n",
              " '[',\n",
              " '5',\n",
              " ']',\n",
              " 'NLTK',\n",
              " 'includes',\n",
              " 'graphical',\n",
              " 'demonstrations',\n",
              " 'and',\n",
              " 'sample',\n",
              " 'data',\n",
              " '.',\n",
              " 'It',\n",
              " 'is',\n",
              " 'accompanied',\n",
              " 'by',\n",
              " 'a',\n",
              " 'book',\n",
              " 'that',\n",
              " 'explains',\n",
              " 'the',\n",
              " 'underlying',\n",
              " 'concepts',\n",
              " 'behind',\n",
              " 'the',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'tasks',\n",
              " 'supported',\n",
              " 'by',\n",
              " 'the',\n",
              " 'toolkit',\n",
              " ',',\n",
              " '[',\n",
              " '6',\n",
              " ']',\n",
              " 'plus',\n",
              " 'a',\n",
              " 'cookbook',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "G0ESi6pcg64O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords=nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waKj0m8cg67Z",
        "outputId": "00ee5507-f710-404c-aef4-939f289477c4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stopwords.fileids()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUaDEFI6g6-2",
        "outputId": "33ea7371-6ad4-475d-cb4b-ab154b02119a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['arabic',\n",
              " 'azerbaijani',\n",
              " 'basque',\n",
              " 'bengali',\n",
              " 'catalan',\n",
              " 'chinese',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'greek',\n",
              " 'hebrew',\n",
              " 'hinglish',\n",
              " 'hungarian',\n",
              " 'indonesian',\n",
              " 'italian',\n",
              " 'kazakh',\n",
              " 'nepali',\n",
              " 'norwegian',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'slovene',\n",
              " 'spanish',\n",
              " 'swedish',\n",
              " 'tajik',\n",
              " 'turkish']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "XuYgSRuRhT9F"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)\n",
        "    words"
      ],
      "metadata": {
        "id": "TCTi_2YkhT_8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "4NSlP-xZhUDj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "QnYlo6bJh8iT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "PBy59F0Jh8lQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "gzk2mfm8h8oB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOsQQlAbh8rF",
        "outputId": "6ce79e4a-a08d-40f0-985e-335af617abf9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' ' .join(words)"
      ],
      "metadata": {
        "id": "l3HTTmzQh8uo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQpduYiUiO-w",
        "outputId": "4a7821c5-9937-4a07-b84a-5e526c5156ce"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It',\n",
              " 'accompanied',\n",
              " 'book',\n",
              " 'explains',\n",
              " 'underlying',\n",
              " 'concept',\n",
              " 'behind',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'task',\n",
              " 'supported',\n",
              " 'toolkit',\n",
              " ',',\n",
              " '[',\n",
              " '6',\n",
              " ']',\n",
              " 'plus',\n",
              " 'cookbook',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenizer = nltk.word_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "IAMgvWJYiPBi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3Tu8R4aiPFG",
        "outputId": "c78109e3-ab29-4ea3-a17f-2ab3a57f7a19"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos = nltk.pos_tag(word_tokenizer)"
      ],
      "metadata": {
        "id": "wOl82PZKiVG9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mmv-vcPuiVJ7",
        "outputId": "cba2fb63-0fed-4a51-f5a0-b40031f0d482"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('Natural', 'NNP'),\n",
              " ('Language', 'NNP'),\n",
              " ('Toolkit', 'NNP'),\n",
              " (',', ','),\n",
              " ('or', 'CC'),\n",
              " ('more', 'JJR'),\n",
              " ('commonly', 'RB'),\n",
              " ('NLTK', 'NNP'),\n",
              " (',', ','),\n",
              " ('is', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('suite', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('libraries', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('programs', 'NNS'),\n",
              " ('for', 'IN'),\n",
              " ('symbolic', 'JJ'),\n",
              " ('and', 'CC'),\n",
              " ('statistical', 'JJ'),\n",
              " ('natural', 'JJ'),\n",
              " ('language', 'NN'),\n",
              " ('processing', 'NN'),\n",
              " ('(', '('),\n",
              " ('NLP', 'NNP'),\n",
              " (')', ')'),\n",
              " ('for', 'IN'),\n",
              " ('English', 'NNP'),\n",
              " ('written', 'VBN'),\n",
              " ('in', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('Python', 'NNP'),\n",
              " ('programming', 'NN'),\n",
              " ('language', 'NN'),\n",
              " ('.', '.'),\n",
              " ('It', 'PRP'),\n",
              " ('supports', 'VBZ'),\n",
              " ('classification', 'NN'),\n",
              " (',', ','),\n",
              " ('tokenization', 'NN'),\n",
              " (',', ','),\n",
              " ('stemming', 'VBG'),\n",
              " (',', ','),\n",
              " ('tagging', 'VBG'),\n",
              " (',', ','),\n",
              " ('parsing', 'NN'),\n",
              " (',', ','),\n",
              " ('and', 'CC'),\n",
              " ('semantic', 'JJ'),\n",
              " ('reasoning', 'NN'),\n",
              " ('functionalities', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('[', '$'),\n",
              " ('4', 'CD'),\n",
              " (']', 'NN'),\n",
              " ('It', 'PRP'),\n",
              " ('was', 'VBD'),\n",
              " ('developed', 'VBN'),\n",
              " ('by', 'IN'),\n",
              " ('Steven', 'NNP'),\n",
              " ('Bird', 'NNP'),\n",
              " ('and', 'CC'),\n",
              " ('Edward', 'NNP'),\n",
              " ('Loper', 'NNP'),\n",
              " ('in', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('Department', 'NNP'),\n",
              " ('of', 'IN'),\n",
              " ('Computer', 'NNP'),\n",
              " ('and', 'CC'),\n",
              " ('Information', 'NNP'),\n",
              " ('Science', 'NNP'),\n",
              " ('at', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('University', 'NNP'),\n",
              " ('of', 'IN'),\n",
              " ('Pennsylvania', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('[', 'CC'),\n",
              " ('5', 'CD'),\n",
              " (']', 'JJ'),\n",
              " ('NLTK', 'NNP'),\n",
              " ('includes', 'VBZ'),\n",
              " ('graphical', 'JJ'),\n",
              " ('demonstrations', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('sample', 'JJ'),\n",
              " ('data', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('It', 'PRP'),\n",
              " ('is', 'VBZ'),\n",
              " ('accompanied', 'VBN'),\n",
              " ('by', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('book', 'NN'),\n",
              " ('that', 'WDT'),\n",
              " ('explains', 'VBZ'),\n",
              " ('the', 'DT'),\n",
              " ('underlying', 'JJ'),\n",
              " ('concepts', 'NNS'),\n",
              " ('behind', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('language', 'NN'),\n",
              " ('processing', 'NN'),\n",
              " ('tasks', 'NNS'),\n",
              " ('supported', 'VBN'),\n",
              " ('by', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('toolkit', 'NN'),\n",
              " (',', ','),\n",
              " ('[', 'VBZ'),\n",
              " ('6', 'CD'),\n",
              " (']', 'NN'),\n",
              " ('plus', 'CC'),\n",
              " ('a', 'DT'),\n",
              " ('cookbook', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('tagsets')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpPT_8QsiVNW",
        "outputId": "b68922ee-b93d-4484-9758-ee352251f3cb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.help.upenn_tagset('VBG')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gy2PrytOiesS",
        "outputId": "8700475f-1857-469d-c7a3-c5e68b5b0c56"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n"
          ]
        }
      ]
    }
  ]
}